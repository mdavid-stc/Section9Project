---
title: 'Milestone Report #1'
author: "Mark David"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Training/DataScience/R/Capstone")
set.seed(6844)
library(dplyr)
library(tidytext)
library(stringr)
library(ggplot2)

### Sampling
# Use rbinom below to create a random subsample of the data
pct <- 100   # percent to choose (0-1)
```

This is the first Milestone Report for the Capstone Project of the Data Science Specialization from Johns Hopkins University through Coursera.

The given task is to analyze a corpus of unstructured text and build a word prediction model based on that text. This model will then be instantiated as a web application so that it can be used, but it is understood that the end goal is for the predictor to fit in a cell phone.

The data comes in three data source files in each of four languages. This project deals with the English versions only, but includes all three sources (blogs, news, and twitter).

This report will demonstrate that the data has been loaded, cleaned, and analyzed. The plan for continuing on toward building the model is presented.

All code for this report can be found here: 

## Setup

The setup code simply checks that the source data files are available (and downloads them if they are not already local), loads the profanity list, and establishes some helpful functions.

```{r load, echo=FALSE}
# Check for the presence of the data, possibly zipped.
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zipfn <- "Coursera-SwiftKey.zip"
stopfile <- "final/en_US/stopwords.txt"
blogfn <- "final/en_US/en_US.blogs.txt"
newsfn <- "final/en_US/en_US.news.txt"
twitfn <- "final/en_US/en_US.twitter.txt"
tidyblogfn <- "data/tidy_blogs.rda"
tidynewsfn <- "data/tidy_news.rda"
tidytwitfn <- "data/tidy_twitter.rda"
# Sizes are from "wc -l"
blogsz <-  898288
newssz <- 1010242
twitsz <- 2360148

# Make sure the data is available in the current directory.
if (!file.exists(blogfn) || !file.exists(newsfn) || !file.exists(twitfn)) {
    if (!file.exists(zipfn)) {
        # Download the data file
        download.file(fileUrl, zipfn, "curl")
    }
    # Unzip the data file
    unzip(zipfn)
}

### Tranformations

# Fix some punctuation that messes up the tokenization
cleanText <- function(vec) {
    for (i in 1:length(vec)) {
        line <- iconv(vec[i], "UTF-8", "UTF-8")
        line <-  gsub("[\u201C\u201D\u201E\u201F\u2033\u2036]", '"', line)   # "smart" quotes
        line <-  gsub("[\u2018\u2019\u201A\u201B\u2032\u2035]", "'", line)   # "smart" quotes
        vec[i] <- line
    }
    vec
}

# Insert space between columns for readability
spacedHeaders <- function(n, wc) {   # wc = "with count"
    if (wc)
        c(str_pad("Blogs", n, "left"), "cnt", str_pad("News", n, "left"), "cnt", str_pad("Twitter", n, "left"), "cnt")
    else
        c(str_pad("Blogs", n, "left"), str_pad("News", n, "left"), str_pad("Twitter", n, "left"))
}

# Read stopwords file
#   Could use data("stop_words") {tidytext}
con <- file(stopfile, open="r")
shitlist <- readLines(con)
close(con)
# Prep the profanity list
shitlist <- tidy(shitlist)
colnames(shitlist) <- c("word")   # to match the unnest_tokens output, for joining
```

## Tokenize the Data

When the data is read in, we check that it did not quit partway through. This kind of error can happen when unexpected (control, non-graphical) characters are encountered.

Some extended chars are converted to their ASCII equivalents (such as "smart quotes"), the text is divided into tokens, lowercased, and most punctuation removed. Then tokens that are profanity are removed. Various statistics are counted across 1-, 2-, and 3-grams.

Due to memory constraints, the data files were loaded one at a time, summary statistics were kept, and the data was unloaded in preparation for the next file.

```{r blog, echo=FALSE, message = FALSE}
# Do the full processing for each file one at a time, due to memory constraints.

### BLOG DATA ###

# Read blog data file
if (file.exists(tidyblogfn)) {
    load(file=tidyblogfn)
} else {
    con <- file(blogfn, open="r")
    # Encoding as UTF-8 leads to tolower errors.
    linb <- readLines(con, blogsz, encoding="lat", skipNul=TRUE)   # character vector
    close(con)   ## Close the connection when we are done
    if (length(linb) != blogsz)
        print("ERROR! Blogs were read improperly!")
    lenb <- length(linb)
    if (pct < 100.0)   # sample for speed
        linb <- linb[rbinom(length(linb)*pct, length(linb), .5)]
    
    # Get the token list for the blog data.
    tidyb <- cleanText(linb)
    rm(linb)
    tidyb <- tidy(tidyb)
    #tidyb$docId <- as.numeric(rownames(tidyb))
    save(tidyb, file=tidyblogfn)   # store for later quick loading
}

### Tokenization (includes lowercasing)
# Purposely not using stemming, loses too many meanings
tokb <- tidyb %>% unnest_tokens(word, x, token="words") %>% anti_join(shitlist)
rm(tidyb)
totalwordsb <- nrow(tokb)

### Frequency
cntb <- tokb %>% count(word, sort=TRUE) %>% ungroup()
save(cntb, file="data/unigrams_blogs.rda")   # store for later quick loading
rm(tokb)
# Size of cntX is total unique words
uniqwordsb <- nrow(cntb)
# Sum the values in the n column for total word occurrences
#totalwordsb <- (cntb %>% summarize(total=sum(n)))[[1]]   # better match nrow(tokb)
# Common words
comb <- head(cntb, 20)
# Half of text words
wcnt <- 0
halfb <- 1
for (i in 1:nrow(cntb)) {
    wcnt <- wcnt + as.integer(cntb[i,2])
    if (wcnt > totalwordsb/2)
        break
    halfb <- halfb + 1
}
# Word frequency distribution
ggb <- ggplot(cntb, aes(n/totalwordsb*100)) +
    geom_histogram(binwidth=0.001, fill="blue", show.legend = FALSE) +
    xlim(NA, 0.1) +
    scale_y_log10() +
    labs(title="Blogs", x="Occurrence %")
rm(cntb)

### 2-grams
load(file=tidyblogfn)   # restore
# Split the tokens into pairs
cnt2b <- tidyb %>% unnest_tokens(bigram, x, token="ngrams", n=2) %>% count(bigram, sort=TRUE) %>% ungroup()
# Note: Need to have already removed profanity, split on sentences!
rm(tidyb)
save(cnt2b, file="data/bigrams_blogs.rda")   # store for later quick loading
uniqwords2b <- nrow(cnt2b)
# Sum the values in the n column for total bigram occurrences
totalwords2b <- (cnt2b %>% summarize(total=sum(n)))[[1]]
# Common bigrams
com2b <- head(cnt2b, 20)
rm(cnt2b)

### 3-grams
load(file=tidyblogfn)   # restore
# Split the tokens into triples
cnt3b <- tidyb %>% unnest_tokens(trigram, x, token="ngrams", n=3) %>% count(trigram, sort=TRUE) %>% ungroup()
# Note: Need to have already removed profanity, split on sentences!
rm(tidyb)
save(cnt3b, file="data/trigrams_blogs.rda")   # store for later quick loading
uniqwords3b <- nrow(cnt3b)
# Sum the values in the n column for total trigram occurrences
totalwords3b <- (cnt3b %>% summarize(total=sum(n)))[[1]]
# Common trigrams
com3b <- head(cnt3b, 20)
rm(cnt3b)
```


```{r news, echo=FALSE, message = FALSE}
### NEWS DATA ###

# Read news data file
if (file.exists(tidynewsfn)) {
    load(file=tidynewsfn)
} else {
    con <- file(newsfn, open="rb")   # Must read as binary to avoid "incomplete final line" due to SUB
    linn <- readLines(con, newssz, encoding="lat", skipNul=TRUE)
    close(con)
    if (length(linn) != newssz)
        print("ERROR! News was read improperly!")
    lenn <- length(linn)
    if (pct < 100.0)   # sample for speed
        linn <- linn[rbinom(length(linn)*pct, length(linn), .5)]
    
    # Get the token list for the news data.
    tidyn <- cleanText(linn)
    rm(linn)   # Clean up as we go
    tidyn <- tidy(tidyn)
    #tidyn$docId <- as.numeric(rownames(tidyn))
    save(tidyn, file=tidynewsfn)   # store for later quick loading
}

### Tokenization
tokn <- tidyn %>% unnest_tokens(word, x) %>% anti_join(shitlist)
rm(tidyn)
totalwordsn <- nrow(tokn)

### Frequency
cntn <- tokn %>% count(word, sort=TRUE) %>% ungroup()
save(cntn, file="data/unigrams_news.rda")
rm(tokn)
# Size of cntX is total unique words
uniqwordsn <- nrow(cntn)
# Sum the values in the n column for total word occurrences
totalwordsn <- (cntn %>% summarize(total=sum(n)))[[1]]
# Common words
comn <- head(cntn, 20)
# Half of text words
wcnt <- 0
halfn <- 1
for (i in 1:nrow(cntn)) {
    wcnt <- wcnt + as.integer(cntn[i,2])
    if (wcnt > totalwordsn/2)
        break
    halfn <- halfn + 1
}
# Word frequency distribution
ggn <- ggplot(cntn, aes(n/totalwordsn*100)) +
    geom_histogram(binwidth=0.001, fill="pink", show.legend = FALSE) +
    xlim(NA, 0.1) +
    scale_y_log10() +
    labs(title="News", x="Occurrence %")
rm(cntn)

### 2-grams
load(file=tidynewsfn)   # restore
# Split the tokens into pairs
cnt2n <- tidyn %>% unnest_tokens(bigram, x, token="ngrams", n=2) %>% count(bigram, sort=TRUE) %>% ungroup()
# Note: Need to have already removed profanity, split on sentences!
rm(tidyn)
save(cnt2n, file="data/bigrams_news.rda")   # store for later quick loading
uniqwords2n <- nrow(cnt2n)
# Sum the values in the n column for total bigram occurrences
totalwords2n <- (cnt2n %>% summarize(total=sum(n)))[[1]]
# Common bigrams
com2n <- head(cnt2n, 20)
rm(cnt2n)

### 3-grams
load(file=tidynewsfn)   # restore
# Split the tokens into triples
cnt3n <- tidyn %>% unnest_tokens(trigram, x, token="ngrams", n=3) %>% count(trigram, sort=TRUE) %>% ungroup()
# Note: Need to have already removed profanity, split on sentences!
rm(tidyn)
save(cnt3n, file="data/trigrams_news.rda")   # store for later quick loading
uniqwords3n <- nrow(cnt3n)
# Sum the values in the n column for total trigram occurrences
totalwords3n <- (cnt3n %>% summarize(total=sum(n)))[[1]]
# Common trigrams
com3n <- head(cnt3n, 20)
rm(cnt3n)
```


```{r twit, echo=FALSE, message = FALSE}
### TWITTER DATA ###

# Read twitter data file
if (file.exists(tidytwitfn)) {
    load(file=tidytwitfn)
} else {
    con <- file(twitfn, open="r")
    lint <- readLines(con, twitsz, encoding="lat", skipNul=TRUE)
    close(con)
    if (length(lint) != twitsz)
        print("ERROR! Tweets were read improperly!")
    lent <- length(lint)
    if (pct < 100.0)   # sample for speed
        lint <- lint[rbinom(length(lint)*pct, length(lint), .5)]
    
    # Get the token list for the twitter data.
    tidyt <- cleanText(lint)
    rm(lint)
    tidyt <- tidy(tidyt)
    #tidyt$docId <- as.numeric(rownames(tidyt))
    save(tidyt, file=tidytwitfn)   # store for later quick loading
}

### Tokenization
tokt <- tidyt %>% unnest_tokens(word, x) %>% anti_join(shitlist)
rm(tidyt)
totalwordst <- nrow(tokt)

### Frequency
cntt <- tokt %>% count(word, sort=TRUE) %>% ungroup()
save(cntt, file="data/unigrams_twitter.rda")
rm(tokt)
# Size of cntX is total unique words
uniqwordst <- nrow(cntt)
# Sum the values in the n column for total word occurrences
#totalwordst <- (cntt %>% summarize(total=sum(n)))[[1]]
# Common words
comt <- head(cntt, 20)
# Half of text words
wcnt <- 0
halft <- 1
for (i in 1:nrow(cntt)) {
    wcnt <- wcnt + as.integer(cntt[i,2])
    if (wcnt > totalwordst/2)
        break
    halft <- halft + 1
}
# Word frequency distribution
ggt <- ggplot(cntt, aes(n/totalwordst*100)) +
    geom_histogram(binwidth=0.001, fill="yellow", show.legend = FALSE) +
    xlim(NA, 0.1) +
    scale_y_log10() +
    labs(title="Twitter", x="Occurrence %")
rm(cntt)

### 2-grams
load(file=tidytwitfn)   # restore
# Split the tokens into pairs
cnt2t <- tidyt %>% unnest_tokens(bigram, x, token="ngrams", n=2) %>% count(bigram, sort=TRUE) %>% ungroup()
# Note: Need to have already removed profanity, split on sentences!
rm(tidyt)
save(cnt2t, file="data/bigrams_twitter.rda")   # store for later quick loading
uniqwords2t <- nrow(cnt2t)
# Sum the values in the n column for total bigram occurrences
totalwords2t <- (cnt2t %>% summarize(total=sum(n)))[[1]]
# Common bigrams
com2t <- head(cnt2t, 20)
rm(cnt2t)

### 3-grams
load(file=tidytwitfn)   # restore
# Split the tokens into triples
cnt3t <- tidyt %>% unnest_tokens(trigram, x, token="ngrams", n=3) %>% count(trigram, sort=TRUE) %>% ungroup()
# Note: Need to have already removed profanity, split on sentences!
rm(tidyt)
save(cnt3t, file="data/trigrams_twitter.rda")   # store for later quick loading
uniqwords3t <- nrow(cnt3t)
# Sum the values in the n column for total trigram occurrences
totalwords3t <- (cnt3t %>% summarize(total=sum(n)))[[1]]
# Common trigrams
com3t <- head(cnt3t, 20)
rm(cnt3t)
```


## Exploratory Data Analysis

First, let's look at basic summaries of the three files.

```{r xplor1, echo=FALSE}
linecnt <- c(lenb, lenn, lent)
uniqwords <- c(uniqwordsb, uniqwordsn, uniqwordst)
totalwords <- c(totalwordsb, totalwordsn, totalwordst)
uniqwords2 <- c(uniqwords2b, uniqwords2n, uniqwords2t)
totalwords2 <- c(totalwords2b, totalwords2n, totalwords2t)
uniqwords3 <- c(uniqwords3b, uniqwords3n, uniqwords3t)
totalwords3 <- c(totalwords3b, totalwords3n, totalwords3t)
summ <- data.frame(linecnt, totalwords, uniqwords, totalwords2, uniqwords2, totalwords3, uniqwords3, row.names=spacedHeaders(10, FALSE))
colnames(summ) <- c("Lines", "Total Words", "Unique Words", "Total 2-grams", "Unique 2-grams", "Total 3-grams", "Unique 3-grams")
t(summ)
```

### Most Common Words

Here are the lists of the most common words in each data source, along with counts of how often they occur throughout each file. As expected, **"the"** is the most common word in each, although we can see that Twitter users often skip this word due to the length constraints of that app.

```{r common, echo=FALSE}
common <- cbind(comb, comn, comt)
colnames(common) <- spacedHeaders(10, TRUE)
common
```

From the total word counts shown previously, we can calculate how many unique words make up 50% of the text by summing the occurrence counts of the most common words until we reach half of the total count.

```{r half, echo=FALSE}
halftext <- rbind(halfb, halfn, halft)
colnames(halftext) <- c("Word Count: 50% of Text")
rownames(halftext) <- spacedHeaders(4, FALSE)
halftext
```

We can also see the most common bi-grams.

```{r common2, echo=FALSE}
common2 <- cbind(com2b, com2n, com2t)
colnames(common2) <- spacedHeaders(15, TRUE)
common2
```

And the most common tri-grams.

```{r common3, echo=FALSE}
common3 <- cbind(com3b, com3n, com3t)
colnames(common3) <- spacedHeaders(20, TRUE)
common3
```


### Frequency Graphs

Let's look at the distribution of the most frequent terms, scaled to the percentage of the total text that they represent.
(Note that the graphs are logarithmic on the Y axis.)

```{r graph, echo=FALSE, warning=FALSE}
ggb
ggn
ggt
```

The graphs all show (as expected) that the majority of words occur rarely (the tall left side of the graphs), with only a very low number of words occurring even 0.1% of the time (larger X values have been truncated for graph readability).

### Problems and Next Steps

For the purposes of this project, there are basically an infinite number of words in the language, so we cannot hope to represent them all in a model. Consequently, we will take a two-pronged approach to make the problem more tractable:

1. Use techniques like word stemming/lemmatization to have most entries in our lists cover more than one form of each word.
2. Treat all words that are uncommon as essentially identical from a math point-of-view.

## Modeling

We will build an n-gram model for predicting the next word based on the previous N words. This plan immediately poses two questions:

1. What is N?
2. What happens when we do not have N previous words?

This solution involves multiple models, each brought to bear at an appropriate time.

### Zero-gram "model"

First, before any terms have been typed, there is a base frequency of the most common terms that start a message. This could be thought of as a 0-gram model, although it is not really a model; it's simply a measurement of the most common terms. The top 3 of these can be suggested immediately at the start of a typing session. Since this is a small, static list of terms, determined in advance, it could be maintained on the client side (i.e. on the user's phone) and updated as we learn what terms this individual usually starts messages with.

This could also be 24 lists of 3 terms, one for each letter, triggered by whichever letter the user types first.

### The Main Algorithm: 1/2/3-gram models

As words are typed, we can use our knowledge of the patterns in the training data to predict which words are most likely to be next. We will suggest 3 words for the user to choose from at each stage:

* If there are 3 (or more) terms already typed, use our 3-gram model to predict the following term.
* If there are only 2 terms already typed, or if the 3-gram does not exist in our model above, use our 2-gram model to predict the following term.
* If there is only 1 term already typed, or if any longer (2 or 3) phrases do not exist in our models above, use our unigram model to predict the following term.

### Unknown terms

The existence of terms that are not covered by our model will force adjustments to the main algorithm above.

* In the same way that we formed the "zero-gram" model above, we will have a predetermined list of the most common second terms, regardless of what the first term is. This list will be suggested whenever the previous term was an unknown.
* If an unknown term is either 2 or 3 terms ago (i.e. "\<unknown> \<known>" or "\<unknown> \<known> \<known>"), we will use the N-1 model for prediction. That is, when the unknown term is 2 terms ago, use the unigram model; when the unknown term is 3 terms ago, use the 2-gram model, based on just the last two known terms. The plan simply follows the main algorithm with the unknown term representing a "restart" of the text.

### Saving space

To conserve memory, all terms/bigrams/trigrams with low frequency will be assumed to have the same (small) occurrence count, so we will not have to store statistics for them. There will be a minimum probability of occurrence (calculated ahead of time) that can be assumed for any arbitrary term that is not directly represented in a model.

### Evaluation

In order to measure the validity of the models, the given source data will be split into training vs. testing groups. The models will be built on the training data, and then we will run the models on the testing data (one sentence at a time) to measure how often we can predict the next word in the sentence.

